# å¯¼å…¥ä¾èµ–
from llama_index.core import SimpleDirectoryReader,VectorStoreIndex,StorageContext,load_index_from_storage
from llama_index.embeddings.dashscope import DashScopeEmbedding,DashScopeTextEmbeddingModels
from llama_index.llms.dashscope import DashScope
# è¿™ä¸¤è¡Œä»£ç æ˜¯ç”¨äºæ¶ˆé™¤ WARNING è­¦å‘Šä¿¡æ¯ï¼Œé¿å…å¹²æ‰°é˜…è¯»å­¦ä¹ ï¼Œç”Ÿäº§ç¯å¢ƒä¸­å»ºè®®æ ¹æ®éœ€è¦æ¥è®¾ç½®æ—¥å¿—çº§åˆ«
import logging
logging.basicConfig(level=logging.ERROR)
from llama_index.llms.openai_like import OpenAILike
import os

def indexing(document_path="./docs", persist_path="knowledge_base/test"):
    """
    å»ºç«‹ç´¢å¼•å¹¶æŒä¹…åŒ–å­˜å‚¨
    å‚æ•°
      path(str): æ–‡æ¡£è·¯å¾„
    """
    index = create_index(document_path)
    # æŒä¹…åŒ–ç´¢å¼•ï¼Œå°†ç´¢å¼•ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶
    index.storage_context.persist(persist_path)

def create_index(document_path="./docs"):
    """
    å»ºç«‹ç´¢å¼•
    å‚æ•°
      path(str): æ–‡æ¡£è·¯å¾„
    """
    # è§£æ ./docs ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£
    documents = SimpleDirectoryReader(document_path).load_data()
    # å»ºç«‹ç´¢å¼•
    index = VectorStoreIndex.from_documents(
        documents,
        # æŒ‡å®šembedding æ¨¡å‹
        embed_model=DashScopeEmbedding(
            # ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨é˜¿é‡Œäº‘æä¾›çš„å…¶å®ƒembeddingæ¨¡å‹ï¼šhttps://help.aliyun.com/zh/model-studio/getting-started/models#3383780daf8hw
            model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2
        )
    )
    return index

def load_index(persist_path="knowledge_base/test"):
    """
    åŠ è½½ç´¢å¼•
    å‚æ•°
      persist_path(str): ç´¢å¼•æ–‡ä»¶è·¯å¾„
    è¿”å›
      VectorStoreIndex: ç´¢å¼•å¯¹è±¡
    """
    storage_context = StorageContext.from_defaults(persist_dir=persist_path)
    return load_index_from_storage(storage_context,embed_model=DashScopeEmbedding(
      model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2
    ))

def create_query_engine(index):
    """
    åˆ›å»ºæŸ¥è¯¢å¼•æ“
    å‚æ•°
      index(VectorStoreIndex): ç´¢å¼•å¯¹è±¡
    è¿”å›
      QueryEngine: æŸ¥è¯¢å¼•æ“å¯¹è±¡
    """
    
    query_engine = index.as_query_engine(
      # è®¾ç½®ä¸ºæµå¼è¾“å‡º
      streaming=True,
      # æ­¤å¤„ä½¿ç”¨qwen-plus-0919æ¨¡å‹ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨é˜¿é‡Œäº‘æä¾›çš„å…¶å®ƒqwençš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼šhttps://help.aliyun.com/zh/model-studio/getting-started/models#9f8890ce29g5u
      llm=OpenAILike(
          model="qwen-plus-0919",
          api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
          api_key=os.getenv("DASHSCOPE_API_KEY"),
          is_chat_model=True
          ))
    return query_engine

def ask(question, query_engine):
    """
    å‘ç­”ç–‘æœºå™¨äººæé—®
    å‚æ•°
      question(str): é—®é¢˜
      query_engine(QueryEngine): æŸ¥è¯¢å¼•æ“å¯¹è±¡
    è¿”å›
      str: å›ç­”
    """
    streaming_response = query_engine.query(question)
    streaming_response.print_response_stream()

def ask_with_debug(question, query_engine):
    # æ›´æ–°æç¤ºæ¨¡æ¿
    update_prompt_template(query_engine=query_engine)

    # è¾“å‡ºé—®é¢˜
    print('=' * 50)  # ä½¿ç”¨ä¹˜æ³•ç”Ÿæˆåˆ†å‰²çº¿
    print(f'ğŸ¤” é—®é¢˜ï¼š{question}')
    print('=' * 50 + '\n')  # ä½¿ç”¨ä¹˜æ³•ç”Ÿæˆåˆ†å‰²çº¿

    # è·å–å›ç­”
    response = query_engine.query(question)

    # è¾“å‡ºå›ç­”
    print('ğŸ¤– å›ç­”ï¼š')
    if hasattr(response, 'print_response_stream') and callable(response.print_response_stream):
        response.print_response_stream()
    else:
        print(str(response))

    # è¾“å‡ºå‚è€ƒæ–‡æ¡£
    print('\n' + '-' * 50)  # ä½¿ç”¨ä¹˜æ³•ç”Ÿæˆåˆ†å‰²çº¿
    print('ğŸ“š å‚è€ƒæ–‡æ¡£ï¼š\n')
    for i, source_node in enumerate(response.source_nodes, start=1):
        print(f'æ–‡æ¡£ {i}:')
        print(source_node)
        print()

    print('-' * 50)  # ä½¿ç”¨ä¹˜æ³•ç”Ÿæˆåˆ†å‰²çº¿

    return response


from llama_index.core import PromptTemplate
def update_prompt_template(
        query_engine,
        qa_prompt_tmpl_str = (
        "ä½ å«å…¬å¸å°èœœï¼Œæ˜¯å…¬å¸çš„ç­”ç–‘æœºå™¨äººã€‚ä½ éœ€è¦ä»”ç»†é˜…è¯»å‚è€ƒä¿¡æ¯ï¼Œç„¶åå›ç­”å¤§å®¶æå‡ºçš„é—®é¢˜ã€‚"
        "æ³¨æ„äº‹é¡¹ï¼š\n"
        "1. æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯è€Œéå…ˆéªŒçŸ¥è¯†æ¥å›ç­”é—®é¢˜ã€‚\n"
        "2. å¦‚æœæ˜¯å·¥å…·å’¨è¯¢ç±»é—®é¢˜ï¼Œè¯·åŠ¡å¿…ç»™å‡ºä¸‹è½½åœ°å€é“¾æ¥ã€‚\n"
        "3. å¦‚æœå‘˜å·¥éƒ¨é—¨æŸ¥è¯¢é—®é¢˜ï¼Œè¯·åŠ¡å¿…æ³¨æ„æœ‰åŒåå‘˜å·¥çš„æƒ…å†µï¼Œå¯èƒ½æœ‰2ä¸ªã€3ä¸ªç”šè‡³æ›´å¤šåŒåçš„äºº\n"
        "ä»¥ä¸‹æ˜¯å‚è€ƒä¿¡æ¯ã€‚"
        "---------------------\n"
        "{context_str}\n"
        "---------------------\n"
        "é—®é¢˜ï¼š{query_str}\nã€‚"
        "å›ç­”ï¼š"
    )):
    """
    ä¿®æ”¹promptæ¨¡æ¿
    è¾“å…¥æ˜¯promptä¿®æ”¹å‰çš„query_engineï¼Œä»¥åŠæç¤ºè¯æ¨¡æ¿ï¼›è¾“å‡ºæ˜¯promptä¿®æ”¹åçš„query_engine
    """
    qa_prompt_tmpl_str = qa_prompt_tmpl_str
    qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)
    query_engine.update_prompts(
        {"response_synthesizer:text_qa_template": qa_prompt_tmpl}
    )
    # print("æç¤ºè¯æ¨¡æ¿ä¿®æ”¹æˆåŠŸ")
    return query_engine
